[strings]
# Mode : train, test, serve
mode = train

#os.getcwd() = 'D:\\Workspace\\unibo\\nlp\\chatbot_pytorch_seq2seq'
seq_data1k = \train_data\seq_data_1k.csv
seq_data = \train_data\seq_data.csv

train_data = train_data

#训练集原始文件
resource_data1k = \train_data\xiaohuangji1k.conv
resource_data = \train_data\xiaohuangji.conv

#读取识别原始文件中段落和行头的标示

e = E
m = M

model_data = \train_data
            
[ints]
# vocabulary size 
# 	20,000 is a reasonable size
enc_vocab_size = 20000
dec_vocab_size = 20000
embedding_dim=128
max_length=20
# typical options : 128, 256, 512, 1024
layer_size = 256
# dataset size limit; typically none : no limit
# 当模型数据级别没有那么多时，记得修改这个。max_train_data_size
max_train_data_size = 1000
# max_train_data_size = 50000
batch_size = 1
[floats]
#设置最小Loss，当模型loss值达到这个水平后停止训练
min_loss=0.7

